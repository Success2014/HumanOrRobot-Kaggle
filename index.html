<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">

    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-dark.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/print.css" media="print">

    <title>Human or Robot? by Success2014</title>
  </head>

  <body>

    <header>
      <div class="container">
        <h1>Human or Robot?</h1>
        <h2>Kaggle, Facebook Recruiting, Machine Learning, Data Wrangling, R, Python</h2>

        <section id="downloads">
          <a href="https://github.com/Success2014/HumanOrRobot-Kaggle/zipball/master" class="btn">Download as .zip</a>
          <a href="https://github.com/Success2014/HumanOrRobot-Kaggle/tarball/master" class="btn">Download as .tar.gz</a>
          <a href="https://github.com/Success2014/HumanOrRobot-Kaggle" class="btn btn-github"><span class="icon"></span>View on GitHub</a>
        </section>
      </div>
    </header>

    <div class="container">
      <section id="main_content">
        <h1>
<a id="human-or-robot" class="anchor" href="#human-or-robot" aria-hidden="true"><span class="octicon octicon-link"></span></a>Human or Robot?</h1>

<h2>
<a id="introfrom-kaggle" class="anchor" href="#introfrom-kaggle" aria-hidden="true"><span class="octicon octicon-link"></span></a>Intro(<a href="https://www.kaggle.com/c/facebook-recruiting-iv-human-or-bot">From Kaggle</a>)</h2>

<p>In this competition, I was chasing down robots for an online auction site. Human bidders on the site are becoming increasingly frustrated with their inability to win auctions vs. their software-controlled counterparts. As a result, usage from the site's core customer base is plummeting.</p>

<p>In order to rebuild customer happiness, the site owners need to eliminate computer generated bidding from their auctions. They attempt at building a model to identify these bids using behavioral data.</p>

<p>The goal of this competition is to identify online auction bids that are placed by "robots", helping the site owners easily flag these users for removal from their site to prevent unfair auction activity. </p>

<p>To build better model, I investigated how an online robot bidding site (<a href="http://www.bidderrobot.com/">BidderRobot</a>) worked.
You can get a feel how they can help you win an auction by watching their tutorials.</p>

<h2>
<a id="data" class="anchor" href="#data" aria-hidden="true"><span class="octicon octicon-link"></span></a>Data</h2>

<p>The training dataset has the bidder_id, payment_account, address, all of which are hashed to protect customer privacy. Besides, each bidder in the training dataset has been labelled using other data we couldn't see.</p>

<p>We also have a log file, which is about 1 GB in size, recording each bid, bidder_id, auction id, merchandise category, device used by the bidder, time of bid placed, country of the bidder, ip of the bidder, and the URL from which the bidder get to the auction.</p>

<h2>
<a id="defect-of-the-data" class="anchor" href="#defect-of-the-data" aria-hidden="true"><span class="octicon octicon-link"></span></a>Defect of the data</h2>

<ul>
<li>There are 29 bidders in the training dataset having no bidding info in the log file. I have to remove in order to train more accurate models.</li>
<li>There are 70 bidders in the test dataset having no bidding record in the log file.</li>
<li>There are 5 bidders in the training dataset, who are labelled as robots, only have ONE bid record in the bid log file. That means we cannot really learn something from these robots. Or even, my training algorithm may get confused by these robots. Therefore, I removed them from my training dataset. And this actually proved to be very useful.</li>
</ul>

<h2>
<a id="feature-engineering" class="anchor" href="#feature-engineering" aria-hidden="true"><span class="octicon octicon-link"></span></a>Feature Engineering</h2>

<p>The most important part of this problem is feature engineering. I first checked that each bidder actually had unique payment account and address. So these two features in the training dataset were of NO use.</p>

<p>I engineered about 40 features.</p>

<p>Surprisingly, the minimum time difference between bids made by a user is almost useless. Many of the users, no matter human or robot, have 0 as the minimum time difference. Other Kagglers also found this. I think maybe some errors were introduced when time was hashed. However, the median time between a user's bid and that user's previous bid was found to be useful by some Kagglers (<a href="https://www.kaggle.com/c/facebook-recruiting-iv-human-or-bot/forums/t/14628/share-your-secret-sauce">Discussions Here</a>).</p>

<p>The most useful features I found were: the total number of bids, the number of device, the number of ip, median number of bids over an auction, maximum number of bids over an auction, the ratio of device_number to auction_number, the ratio of median bidding number to the ip number, the ratio of total number of bids to the number of auctions participated.</p>

<h2>
<a id="results" class="anchor" href="#results" aria-hidden="true"><span class="octicon octicon-link"></span></a>Results</h2>

<p>My final model was trained using Random Forest with a public score 0.86815 and a private score 0.91158. I also tried stacking several of my models, which resulted in the public score 0.87242 and the private score 0.91263.</p>

<h2>
<a id="final-thoughts" class="anchor" href="#final-thoughts" aria-hidden="true"><span class="octicon octicon-link"></span></a>Final Thoughts</h2>

<ol>
<li><p>During the contest, I didn't make good use the time. If I explored the time deeper, I might end up with being the top.</p></li>
<li>
<p>Here are some offline solutions to the robot bidding problem. </p>

<ul>
<li>We could record the robots' IP, bidder_id, payment account and address. If we encounter them in the future, we could ban them from bidding.</li>
<li>We should run the machine learning algorithm on regular bases to adapt to new robot behaviors.</li>
</ul>
</li>
<li>
<p>From the most important features of the model, we could derive some online solutions. Here are a few.</p>

<ul>
<li>We can set a limit to the maximum number of bidding over an auction. </li>
<li>We can ban users logging in from multiple devices.</li>
<li>We can set a limit to the maximum number of bids a user can place a day.</li>
<li>Record the median time between a user's bid and that user's previous bid. If it is lower than a threshold, flag the user as robot. </li>
</ul>
</li>
</ol>
      </section>
    </div>

              <script type="text/javascript">
            var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
            document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
          </script>
          <script type="text/javascript">
            try {
              var pageTracker = _gat._getTracker("UA-65242930-3");
            pageTracker._trackPageview();
            } catch(err) {}
          </script>

  </body>
</html>
